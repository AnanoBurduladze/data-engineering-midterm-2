1. Redis
a) Redis ძირითადი მონაცემთა ტიპებია : String, list, hash, set და sorted Set.
String – ყველაზე მარტივი მონაცემთა ტიპი, რომელიც ერთ კონკრეტულ მნიშვნელობას ინახავს. ქეში (მაგალითად, ხშირად გამოყენებული მონაცემების სწრაფად დაბრუნება)
List – ელემენტების ჩამონათვალი, სადაც მონაცემები შეიძლება დაემატოს ან წაიშალოს დასაწყისიდან ან ბოლოდან.
განკუთვნილია ისეთი მონაცემების შესანახად, როგორიცაა მაგალითად რიგები და ლოგები.
Set – აერთიანებს უნიკალურ მნიშვნელობებს და ხშირად გამოიყენება, მაგალითად, მომხმარებლების მიერ მონიშნული ელემენტების მართვისთვის.
Sorted Set – ინახავს უნიკალურ მნიშვნელობებს ქულებით (score), რაც სასარგებლოა რეიტინგებისა და ლაივ მონიტორინგისთვის.
Hash – საშუალებას იძლევა, მონაცემები შენახულ იქნას key-value წყვილებად, რაც ოპტიმალურია მომხმარებლის პროფილებისა და კონფიგურაციისთვის.
b) Redis-ს მონაცემთა დაკარგვის თავიდან ასაცილებლად ორი ძირითადი მექანიზმი აქვს:
RDB (Redis Database Backup) – ეს მეთოდი პერიოდულად ქმნის მონაცემთა snapshot-ებს და ინახავს მათ დისკზე.
AOF (Append-Only File) – ყველა ბრძანება ინახება ლოგ ფაილში, რაც უზრუნველყოფს მონაცემთა აღდგენას ავარიული გათიშვის შემთხვევაში.
ამის გარდა, შესაძლებელია ორივე მეთოდის გაერთიანება, რაც იძლევა როგორც შესრულების მაღალ სიჩქარეს, ასევე საიმედოობას.

3. Apache Kafka
a) Kafka-ს არქიტექტურა დაყოფილია რამდენიმე კომპონენტად, რაც მას მაღალწარმად და მტკიცე სისტემად აქცევს:
Broker – დამოუკიდებელი სერვერი, რომელიც ინახავს და აწვდის მონაცემებს.
Topics – ლოგიკური სივრცე, სადაც იწერება შეტყობინებები და რომელიც მომხმარებლებს სხვადასხვა ნაკადში უყოფს მონაცემებს.
Partitions – თითოეული თემა შეიძლება დაყოფილ იქნას სხვადასხვა ნაწილად, რაც უზრუნველყოფს პარალელურ დამუშავებას.
Consumer Groups – მომხმარებლების ჯგუფი, რომელიც ერთობლივად ამუშავებს მონაცემებს და უზრუნველყოფს უფრო ეფექტურ დატვირთვის განაწილებას.
b) Kafka-ს მასშტაბურობა ეფუძნება პარტიციების მექანიზმს, რაც პარალელურად მონაცემთა დამუშავების საშუალებას იძლევა. მაღალ წარმადობას უზრუნველყოფს:
Zero-copy data transfer – მონაცემები პირდაპირ გადაეცემა network buffers-ს.
Batch Processing – Kafka იყენებს batch-ებს, რათა ერთდროულად დამუშავდეს დიდი მოცულობის მონაცემი.
Efficient Disk I/O – მისი სტრუქტურა განკუთვნილია დისკზე სექვენციური ჩაწერისთვის, რაც მნიშვნელოვნად ამცირებს შეფერხებებს.

4. Apache Airflow
a) DAG (Directed Acyclic Graph) წარმოადგენს პროცესების ნაკრებს, სადაც თითოეული ეტაპი განსაზღვრავს დამოკიდებულებებს სხვა ეტაპებთან.
მთავარი პრინციპი: თითოეულ ამოცანას (task) თავისი ადგილი აქვს სქემაში და იგი ვერ განმეორდება (არ წარმოიქმნება ციკლები).
გამოყენების შემთხვევები: ხშირად გამოიყენება მონაცემთა პაიპლაინების, ავტომატური რეპორტინგის და სისტემური ინტეგრაციისთვის.
b) Operators – არის აქტიური ოპერაციები, რომლებიც იყენებენ გარკვეულ ბიბლიოთეკებს ან კოდის ბლოკებს. მაგ., PythonOperator, BashOperator.
Sensors – ესენი პასიურად ელოდებიან რაიმე მოვლენის მოხდენას. მაგალითად, FileSensor შეუძლია მონიტორინგი გაუწიოს კონკრეტული ფაილის გაჩენას სისტემაში.

4. ETL vs ELT
a) ETL (Extract, Transform, Load) – ეს მეთოდი იყენებს მონაცემთა წინასწარ დამუშავებას და მხოლოდ ამის შემდეგ აგზავნის მონაცემებს საცავში.
   ELT (Extract, Load, Transform) – მონაცემები ჯერ იტვირთება საცავში და შემდეგ ხდება მათი დამუშავება, რაც ოპტიმალურია დიდი მოცულობის მონაცემებისთვის.
მთავარი განსხვავებები:
ETL უკეთესია სტრუქტურირებული მონაცემებისთვის და იყენებს მონაცემთა საწყობებს (Data Warehouse).
ELT მეტად მოქნილია და უკეთესად მუშაობს Data Lake გარემოში.
b) ETL ჯობია მაშინ, როცა საჭიროა მონაცემების მაღალი სიზუსტე და სტრუქტურირება (მაგ:საბანკო სექტორი).
   ELT უკეთესია მაშინ, როცა მონაცემები უდიდესი მოცულობისაა და გვჭირდება მონაცემთა სწრაფი დამუშავება (მაგ:stock მარკეტი)
   
5. მონაცემთა შენახვის კონცეფციები
a) Data Lake – შეიცავს ნედლ მონაცემებს, რომლებიც შეიძლება იყოს სტრუქტურირებული, ნახევრად-სტრუქტურირებული ან არასტრუქტურირებული. მას ხშირად იყენებენ Big Data და AI/ML სისტემებში.
   Data Warehouse – შედგება სტრუქტურირებული მონაცემებისგან, რომლებიც არის გაწმენდილი და ოპტიმიზირებული ანალიტიკური ანგარიშებისთვის.
   Data Mart – წარმოადგენს Data Warehouse-ის ნაწილს, რომელიც ფოკუსირებულია კონკრეტულ განყოფილებაზე (მაგალითად, ფინანსებზე, გაყიდვებზე).
b) Data Lake იყენებს ELT მიდგომას, სადაც მონაცემები პირველ რიგში ინახება და შემდეგ მუშავდება საჭიროებისამებრ.
   Data Warehouse ემყარება ETL პროცესს, რაც უზრუნველყოფს მონაცემების სიზუსტეს და ხარისხის კონტროლს.
   Data Mart ხშირად არის Data Warehouse-ის ნაწილი და იყენებს ETL-ს, რათა შეინახოს მხოლოდ საჭირო მონაცემები კონკრეტული ბიზნეს სექტორისთვის.
